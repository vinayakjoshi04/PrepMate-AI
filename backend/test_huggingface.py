# backend/test_huggingface.py
# Test Hugging Face Inference API with new client

import os
from dotenv import load_dotenv

# Import or install Hugging Face client
try:
    from huggingface_hub import InferenceClient
except ImportError:
    print("Installing huggingface_hub...")
    import subprocess
    subprocess.check_call(["pip", "install", "huggingface_hub"])
    from huggingface_hub import InferenceClient

load_dotenv()

HF_API_KEY = os.getenv("HF_API_KEY")

print("=" * 60)
print("ğŸ§ª TESTING HUGGING FACE API (LLAMA)")
print("=" * 60)

if not HF_API_KEY:
    print("âŒ HF_API_KEY not found in .env file!")
    print("\nğŸ“ Get your FREE API key:")
    print("   1. Go to: https://huggingface.co/settings/tokens")
    print("   2. Sign in/Sign up (free)")
    print("   3. Click 'New token'")
    print("   4. Name it 'PrepMate-AI'")
    print("   5. Role: 'read'")
    print("   6. Copy the token")
    print("   7. Add to .env: HF_API_KEY=hf_...")
    exit(1)

print(f"âœ… API Key found: {HF_API_KEY[:20]}...")

# Test with Llama model
MODEL = "meta-llama/Meta-Llama-3.1-8B-Instruct"

print(f"\nğŸ¤– Using model: {MODEL}")
print("ğŸ“¡ Testing API call with new InferenceClient...")

try:
    # Initialize client
    client = InferenceClient(token=HF_API_KEY)
    
    # Test message
    messages = [
        {
            "role": "system",
            "content": "You are a helpful assistant. Respond with valid JSON only."
        },
        {
            "role": "user",
            "content": 'Say "Hello, PrepMate-AI is working with Llama on Hugging Face!" in JSON format: {"message": "...", "status": "success"}'
        }
    ]
    
    print("\nâ³ Calling Hugging Face API...")
    
    response = client.chat_completion(
        messages=messages,
        model=MODEL,
        max_tokens=200,
        temperature=0.7
    )
    
    generated_text = response.choices[0].message.content
    
    print("\nâœ… SUCCESS! Hugging Face API is working!")
    print("\nğŸ“ Response:")
    print(generated_text)
    print("\nğŸ‰ Your backend is ready!")
    print("\nğŸ’¡ Hugging Face Free Tier:")
    print("   â€¢ 1,000 requests/day")
    print("   â€¢ Unlimited models")
    print("   â€¢ No credit card needed")
    
except Exception as e:
    error_msg = str(e)
    
    if "503" in error_msg or "loading" in error_msg.lower():
        print("â³ MODEL IS LOADING")
        print("ğŸ” The model is being loaded by Hugging Face")
        print("\nğŸ’¡ This is normal for the first request!")
        print("   Waiting 20 seconds and retrying...")
        
        import time
        time.sleep(20)
        
        try:
            print("\nğŸ“¡ Retrying...")
            response = client.chat_completion(
                messages=messages,
                model=MODEL,
                max_tokens=200,
                temperature=0.7
            )
            generated_text = response.choices[0].message.content
            print("âœ… SUCCESS on retry!")
            print("\nğŸ“ Response:")
            print(generated_text)
        except Exception as retry_error:
            print(f"âŒ Still failing: {retry_error}")
    
    elif "401" in error_msg or "unauthorized" in error_msg.lower():
        print("âŒ UNAUTHORIZED")
        print("ğŸ” Invalid API key")
        print("\nğŸ’¡ Get a new token at:")
        print("   https://huggingface.co/settings/tokens")
    
    elif "rate limit" in error_msg.lower() or "429" in error_msg:
        print("âŒ RATE LIMIT")
        print("ğŸ” Too many requests")
        print("\nğŸ’¡ Free tier: 1,000 requests/day")
    
    else:
        print(f"âŒ ERROR: {error_msg}")

print("\n" + "=" * 60)